%%% MAIN DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass [12pt]{article}

%%% Packages %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{fancyhdr}

\usepackage{fancyvrb}
\usepackage{relsize}

\usepackage[pdftex]{graphicx}
\usepackage[pdftex]{hyperref}

\hypersetup{colorlinks, bookmarksopen, linkcolor = blue, citecolor = blue}

%%% New formats %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand\vek[1]{\mathbf{#1}}                       % vector denotes bold
\newcommand{\bmath}[1]{\mbox{\boldmath$#1$}}          % denotes bold where first command does not work
\newcommand\mat[1]{\ensuremath{\mathbf{#1}}}          % vector denotes bold without math environment
\newcommand\funct[2]{\ensuremath{{\mathrm#1}({#2})}}  % nevim
\newcommand{\norm}[1]{\|#1\|}                         % euclidean norm
\newcommand{\ii}[0]{{\'{\i}}}
\newcommand{\vt}[0]{t$\!$'}
\newcommand{\complete}[1]{({\it #1})}
\newcommand\uv[1]{``#1''}

\begin{document}

The direct optimization approach to identification of model's parameters in case of several preferences put on the resulting curves can be tackled by three different methodologies: (i) single-objective, (ii) rank-based single objective and (iii) multi-objective.

\section{Single objective identification}

The most traditional way of dealing of multiple objectives in optimization is the Weighted Sum Method. Each objective out of $k$ objectives $f_i(\vek{x})$ is multiplied by user defined weights $w_i$ and their sum is optimized. The problem is converted into single criteria optimization:
%
\begin{equation}\label{MO}
    F(\vek{x}) = \sum_{i=1}^{k} w_i f_i(\vek{x}) , \quad\quad \sum_{i=1}^{k} w_i = 1, \quad w_i \ge 0 ,
\end{equation}
%
and any single-objective optimization method can be used to solve $F(\vek{x})$. Although this method is really easy and intuitive, the biggest obstacle is in setting the weights. The weights express the relative importance of individual objectives, which, in real world applications, is difficult to determine. The success of the WSM depends also on scaling of objectives; all of them should have more or less the same order of magnitude to affect the value of $F(\vek{x})$ similarly. Therefore, not only weight vector must be set, but also a~normalization of objectives must be performed. Similarly to weight vector, it is difficult to determine in advance which objectives' values can be reached and accordingly, to properly set the normalization vector.

It is shown in~\cite{Miettinen:1999:MO} that for a~convex problem the WSM is able to find solutions on the entire Pareto optimal front. Unfortunately, the nonconvex problem are unbeatable for the WSM~\cite{deb:2001}, see Figure~\ref{fig:WSM}. The weight vector determines the slope ($-w_1/w_2$) of lines with same value of $\funct{F}{\vek{x}}$. With moving the line from right to left the $\funct{F}{\vek{x}}$ value is decreasing; the minimum comes with a~line which is tangential to the feasible space, see the left figure. With different weight vectors, all Pareto optimal solutions on left figure can be found. On the contrary, on the right figure, with a~nonconvex shape of the Pareto optimal front, no weight vector can produce a~tangent point within the region $BC$~\cite{deb:2001}.
%
\begin{figure}[!tb]
\centering
         \includegraphics*[width=0.90\textwidth]{Figures/wsm.jpg}

    \caption{The principle of weighted sum method.}
\label{fig:WSM}
\end{figure}
%

As a result, non-convexity of the Pareto front produces multi-modal behaviour. Again, when inspecting Figure~\ref{fig:WSM} (right), two local minima, B and C will be obtained for the shown settings of weights. Therefore, a multi-modal algorithm is recommended that is capable of finding several local minima on the single-objective landscape.

In this contribution, a genetic algorithm called GRADE \cite{Ibrahimbegovic:2004,Kucerova:2007:PHD} was applied. It is a real-coded genetic algorithm combining the ideas of genetic operators: cross-over, mutation and selection taken from the
standard genetic algorithm and the idea of differential operator taken from the differential evolution.
Moreover, the algorithm GRADE is supported with the niching method
CERAF, which was developed based on an idea of enhancing the algorithm with
memory and restarts~\cite{Hrstka:2004:AES}. When the GRADE algorithm loses the convergence, the current position of the optimization algorithm is marked as a local extreme and a forbidden area is build around in order to forbid the optimization algorithm again to fall into the same local extreme. Hence all inspected local extremes are stored in memory and can be inspected after the optimization.
Particularly, the main setting of the optimization procedure was as follows: the population of the genetic algorithm contains 30 independent solutions, the whole identification stops after 20.000 objective function evaluations and a local extreme was marker after 600 evaluations without any improvement.

\section{Rank-based single objective identification}

In case that no knowledge of the objective functions' values is available and hence, the settings of the weights in the WSM methods cannot be set properly, the application of the single-objective approach is not appropriate. One possible solution is so-called Average Ranking (AR) \cite{Leps:2007}, which sums ranks of the objective functions instead of the objective functions' values. Therefore, no weights are needed, however, its is still not an appropriate solution to the general multi-objective problem since the Pareto-dominance is not preserved, see e.g. \cite{vitingerova:2010} for discussion. An application of the AR algorithm to parameters identification has been presented in \cite{Kuraz:2010:JCAM}.

\section{Multi-objective identification}

It seems to be advantageous to use population-based Evolutionary Algorithms to obtain the Pareto set for general multi-objective optimization problem. However, in case of multi-objective identification of models' parameters, the comparison presented in  \cite{vitingerova:2010} showed no clear winner and the selection of the best methodology is problem-dependent.

We see the advantage of multi-objective optimization and/or visualization mainly in ability to {\it a~posteriori} select the final solution from all already visited points. Even in case of single-objective optimization using the WSM, the visualization of the original multi-objective landscape can help to identify and inspect local minima produced by the weighting of the objectives.



\bibliographystyle{apalike}
\bibliography{liter}

\end{document}


